{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "year_list = range(2011,2017,1)\n",
    "cpi = [114.51, 117.76, 119.07, 119.48, 120.15, 120.15]\n",
    "income_list = [\"earns\",\"dispy\",\"tax\",\"sicer\",\"sicee\",\"sicse\",\"ben\"]\n",
    "data_path = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read and merge data\n",
    "\n",
    "silc = pd.read_table(data_path+\"be_2012_a4.txt\",sep='\\t')\n",
    "#print(silc.shape)\n",
    "\n",
    "# for each year in year_list\n",
    "for year in year_list:\n",
    "    # read the tab seperated file\n",
    "    df = pd.read_table(data_path+\"be_%d_std.txt\" %year,sep='\\t')\n",
    "    # keep only id and columns starting with 'ils'\n",
    "    columns = df.columns\n",
    "    keep_columns = [\"idperson\"] + list(columns[columns.str.startswith(\"ils_\")])\n",
    "    df = df[keep_columns]\n",
    "    # add suffix of the year to all the column names except idperson\n",
    "    columns = pd.Series(df.columns)\n",
    "    columns[columns.str.startswith(\"ils_\")] = columns[columns.str.startswith(\"ils_\")] + \"_%d\" %year\n",
    "    df.columns = columns\n",
    "    # merge with silc on idperson\n",
    "    silc = silc.merge(df,on=\"idperson\")\n",
    "    #print(silc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## GENERATE ADDITIONAL VARIABLES AT INDIVIDUAL LEVEL\n",
    "\n",
    "# gross income for calculating household head later\n",
    "silc['tmp_gr_inc'] = -1000 * (silc['dag']<18) + silc['ils_origy_2011'] + 0.0001*np.random.uniform(size=silc.shape[0])\n",
    "    # make sure no minor becomes hhh\n",
    "    #it happens that people in the hh have same gross income -> add random very small amount\n",
    "\n",
    "# indicator for underage \n",
    "silc['ch'] = silc['dag']<18\n",
    "\n",
    "# socio-economic status\n",
    "silc['working']= (silc['dag'] >= 18) & ( (silc['yem'] + silc['yse']) > 200 ) \n",
    "silc['pension']= (silc['dag']  >= 40) & (silc['poa'] > (silc['yem'] + silc['yse'] + silc['bun'])) & (silc['working'] == False)\n",
    "silc['unempl']= (silc['dag']  >= 18) & ( (silc['bun']>0) | (silc['les'] == 5) ) & (silc['working'] == False) & (silc['pension'] == False)\n",
    "silc['inactive']= (silc['dag']  >= 18) & (silc['les'] != 6) & (silc['unempl'] == 0) & (silc['working'] == 0) & (silc['pension'] == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## AGGREGATIONS AT HOUSEHOLD LEVEL\n",
    "\n",
    "# create dictionary to store aggregations at the household level\n",
    "hh_agg_dict = {}\n",
    "\n",
    "for year in year_list:\n",
    "    for var in income_list:\n",
    "        #print(\"ils_%s_%s\" %(var,year))\n",
    "        hh_agg_dict[\"ils_%s_%s\" %(var,year)] = {\"hh_%s_%s\" %(var,year):\"sum\"}\n",
    "\n",
    "hh_agg_dict['idperson'] = {'hh_size': 'count'}\n",
    "hh_agg_dict['ch'] = {'hh_ch':'sum'}\n",
    "hh_agg_dict['tmp_gr_inc'] = {'tmp_gr_max':'max'}\n",
    "\n",
    "# create household measures by grouping and using aggregation dictionary\n",
    "hh_data = silc.groupby('idhh').agg(hh_agg_dict)\n",
    "\n",
    "# drop upper level of multiindex\n",
    "hh_data.columns = hh_data.columns.droplevel(0)\n",
    "\n",
    "# merge household measures back to individual measures\n",
    "silc = silc.merge(hh_data,left_on='idhh',right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# indicator for household head\n",
    "silc['hhh'] = silc['tmp_gr_inc'] == silc['tmp_gr_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EQUIVALENCE\n",
    "\n",
    "# create equivalence scale\n",
    "tmp = 1 * silc['hhh']==1  #assign 1 to the oldest person in the hh\n",
    "tmp[(silc['dag']>=14) & (tmp==0)] = 0.5 # other adults 0.5\n",
    "tmp[(silc['dag'] < 14)  & (tmp == 0)] = 0.3 # and children 0.3\n",
    "\n",
    "# calculate sum of equivalence scale by household\n",
    "silc['tmp'] = tmp\n",
    "eq_scale = silc.groupby('idhh')['tmp'].sum()\n",
    "\n",
    "# merge sum back to silc\n",
    "silc = silc.merge(pd.DataFrame(data=eq_scale).rename(columns={\"tmp\":\"eq_scale\"}),left_on='idhh',right_index=True)\n",
    "\n",
    "# create equivalent disposable income\n",
    "for year in year_list:\n",
    "    silc['eq_dispy_%s' %year] = silc['hh_dispy_%s' %year] / silc['eq_scale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weights corrected for household size\n",
    "silc['w_hh'] = silc['dwt'] * silc['hh_size']\n",
    "silc['w_hh'] = silc['w_hh'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "silc.to_pickle(\"data/be_em_2011_2016\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "silc = pd.read_pickle(\"data/be_em_2011_2016\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create subgroups (deciles, ventiles, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_quantile = 10 # number of quantiles\n",
    "var_q = \"eq_dispy_2011\" # variable to be used in ordering\n",
    "var_x = \"hh_dispy_2011\" # variable on x-axis\n",
    "var_y = \"drgn1\" # variable on y-axis\n",
    "group = \"drgn1\"# grouping over a factor variable (drgn1 = region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for weighted quantiles from http://stackoverflow.com/questions/21844024/weighted-percentile-using-numpy\n",
    "\n",
    "def weighted_quantile(values, quantiles, sample_weight=None, values_sorted=False, old_style=False):\n",
    "    \"\"\" Very close to numpy.percentile, but supports weights.\n",
    "    NOTE: quantiles should be in [0, 1]!\n",
    "    :param values: numpy.array with data\n",
    "    :param quantiles: array-like with many quantiles needed\n",
    "    :param sample_weight: array-like of the same length as `array`\n",
    "    :param values_sorted: bool, if True, then will avoid sorting of initial array\n",
    "    :param old_style: if True, will correct output to be consistent with numpy.percentile.\n",
    "    :return: numpy.array with computed quantiles.\n",
    "    \"\"\"\n",
    "    values = np.array(values)\n",
    "    quantiles = np.array(quantiles)\n",
    "    if sample_weight is None:\n",
    "        sample_weight = np.ones(len(values))\n",
    "    sample_weight = np.array(sample_weight)\n",
    "    assert np.all(quantiles >= 0) and np.all(quantiles <= 1), 'quantiles should be in [0, 1]'\n",
    "\n",
    "    if not values_sorted:\n",
    "        sorter = np.argsort(values)\n",
    "        values = values[sorter]\n",
    "        sample_weight = sample_weight[sorter]\n",
    "\n",
    "    weighted_quantiles = np.cumsum(sample_weight) - 0.5 * sample_weight\n",
    "    if old_style:\n",
    "        # To be convenient with numpy.percentile\n",
    "        weighted_quantiles -= weighted_quantiles[0]\n",
    "        weighted_quantiles /= weighted_quantiles[-1]\n",
    "    else:\n",
    "        weighted_quantiles /= np.sum(sample_weight)\n",
    "    return np.interp(quantiles, weighted_quantiles, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct deciles of individuals in the population ranked by equivalised disposable household income\n",
    "\n",
    "# make list of quantiles to compute\n",
    "my_quantiles = [x/float(100) for x in range(10,101,n_quantile)]\n",
    "\n",
    "# create empty dictionary that will store the quantiles as lists as values and the groups as keys\n",
    "quantiles_by_group = {}\n",
    "\n",
    "# keep frequency weights of rows with household heads\n",
    "silc_hh = silc[silc[\"hhh\"]==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each group in the grouping variable\n",
    "for groupi in np.unique(silc[group]):\n",
    "    # keep only the lines of silc_hh from that group\n",
    "    silc_use = silc_hh[(silc_hh[group]==groupi)]\n",
    "    # calculate the weighted quantiles and store in the dictionary\n",
    "    quantiles_by_group[groupi] = weighted_quantile(values = silc_use[var_q],\n",
    "                                                   quantiles = my_quantiles, \n",
    "                                                   sample_weight = silc_use[\"w_hh\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([  688.85222782,   870.8705372 ,  1001.70984375,  1147.35076481,\n",
       "         1312.52625866,  1508.53921575,  1767.46409967,  2101.34608853,\n",
       "         2701.62450519,  8221.48      ]),\n",
       " 2: array([  1010.68208622,   1216.22762584,   1383.7313368 ,   1555.75722476,\n",
       "          1699.53260646,   1857.86997718,   2035.25644243,   2248.08777178,\n",
       "          2579.3265604 ,  16950.99      ]),\n",
       " 3: array([   888.08120702,   1091.46039012,   1231.91110199,   1387.95213599,\n",
       "          1549.06045381,   1689.46782609,   1846.50406115,   2057.56996441,\n",
       "          2366.71811072,  11781.99      ])}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantiles_by_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## CHART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
